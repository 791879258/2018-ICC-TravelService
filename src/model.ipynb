{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import catboost as cb\n",
    "import time\n",
    "import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def merge_feature(\n",
    "    act_type_window,\n",
    "    act_type_num_window,\n",
    "    act_type_rate_window,\n",
    "    act_type_row_stat_window,\n",
    "    act_time_window,\n",
    "    act_time_1type_window,\n",
    "    act_ord_act_time_diff_window,\n",
    "    action_sequence_time_diff_window,\n",
    "    action_time_diff_234_56789_window,\n",
    "    action_time_diff_stat_window\n",
    "):\n",
    "    util.log('Merge feature...')\n",
    "    \n",
    "    order_future_tr = pd.read_csv('../data/input/train/orderFuture_train.csv')\n",
    "    order_future_te = pd.read_csv('../data/input/test/orderFuture_test.csv')\n",
    "\n",
    "    user_profile = pd.read_csv('../data/output/feat/%s' % 'user_profile')\n",
    "    train = pd.merge(order_future_tr, user_profile, on='userid', how='left')\n",
    "    test = pd.merge(order_future_te, user_profile, on='userid', how='left')\n",
    "    \n",
    "    user_comment = pd.read_csv('../data/output/feat/%s' % 'user_comment')\n",
    "    train = pd.merge(train, user_comment, on='userid', how='left')\n",
    "    test = pd.merge(test, user_comment, on='userid', how='left')\n",
    "    \n",
    "    order_history = pd.read_csv('../data/output/feat/%s' % 'order_history')\n",
    "    train = pd.merge(train, order_history, on='userid', how='left')\n",
    "    test = pd.merge(test, order_history, on='userid', how='left')\n",
    "    \n",
    "#     order_history_last_w = pd.read_csv('../data/output/feat/%s' % 'order_history_last_w')\n",
    "#     train = pd.merge(train, order_history_last_w, on='userid', how='left')\n",
    "#     test = pd.merge(test, order_history_last_w, on='userid', how='left')\n",
    "    \n",
    "    action_type = pd.read_csv('../data/output/feat/%s' % 'action_type')\n",
    "    train = pd.merge(train, action_type, on='userid', how='left')\n",
    "    test = pd.merge(test, action_type, on='userid', how='left')\n",
    "    \n",
    "    action_type_based_on_time = pd.read_csv('../data/output/feat/%s' % 'action_type_based_on_time')\n",
    "    train = pd.merge(train, action_type_based_on_time, on='userid', how='left')\n",
    "    test = pd.merge(test, action_type_based_on_time, on='userid', how='left')\n",
    "    \n",
    "    util.log('act_type_window=' + str(act_type_window))\n",
    "    window = act_type_window\n",
    "    action_type_based_on_time_last_window = pd.read_csv('../data/output/feat/%s%d' % ('action_type_based_on_time_last_window', window))\n",
    "    train = pd.merge(train, action_type_based_on_time_last_window, on='userid', how='left')\n",
    "    test = pd.merge(test, action_type_based_on_time_last_window, on='userid', how='left')\n",
    "    \n",
    "    util.log('act_type_num_window=' + str(act_type_num_window))\n",
    "    window = act_type_num_window\n",
    "    action_type_num_based_on_time_last_window = pd.read_csv('../data/output/feat/%s%d' % ('action_type_num_based_on_time_last_window', window))\n",
    "    train = pd.merge(train, action_type_num_based_on_time_last_window, on='userid', how='left')\n",
    "    test = pd.merge(test, action_type_num_based_on_time_last_window, on='userid', how='left')\n",
    "    \n",
    "    util.log('act_type_rate_window=' + str(act_type_rate_window))\n",
    "    window = act_type_rate_window\n",
    "    action_type_rate_based_on_time_last_window = pd.read_csv('../data/output/feat/%s%d' % ('action_type_rate_based_on_time_last_window', window))\n",
    "    train = pd.merge(train, action_type_rate_based_on_time_last_window, on='userid', how='left')\n",
    "    test = pd.merge(test, action_type_rate_based_on_time_last_window, on='userid', how='left')\n",
    "    \n",
    "    util.log('act_type_row_stat_window=' + str(act_type_row_stat_window))\n",
    "    window = act_type_row_stat_window\n",
    "    action_type_row_stat_based_on_time_last_window_feat = pd.read_csv('../data/output/feat/%s%d' % ('action_type_row_stat_based_on_time_last_window', window))\n",
    "    train = pd.merge(train, action_type_row_stat_based_on_time_last_window_feat, on='userid', how='left')\n",
    "    test = pd.merge(test, action_type_row_stat_based_on_time_last_window_feat, on='userid', how='left')\n",
    "    \n",
    "#     util.log('action_num_window=' + str(action_num_window))\n",
    "#     window = action_num_window\n",
    "#     action_num_based_on_time_last_window = pd.read_csv('../data/output/feat/%s%d' % ('action_num_based_on_time_last_window', window))\n",
    "#     train = pd.merge(train, action_num_based_on_time_last_window, on='userid', how='left')\n",
    "#     test = pd.merge(test, action_num_based_on_time_last_window, on='userid', how='left')\n",
    "\n",
    "#     util.log('action_type_num_window=' + str(action_type_num_window))\n",
    "#     window = action_type_num_window\n",
    "#     action_type_num_based_on_time_last_window = pd.read_csv('../data/output/feat/%s%d' % ('action_type_num_based_on_time_last_window', window))\n",
    "#     train = pd.merge(train, action_type_num_based_on_time_last_window, on='userid', how='left')\n",
    "#     test = pd.merge(test, action_type_num_based_on_time_last_window, on='userid', how='left')\n",
    "\n",
    "    action_time_based_on_time = pd.read_csv('../data/output/feat/%s' % 'action_time_based_on_time')\n",
    "    train = pd.merge(train, action_time_based_on_time, on='userid', how='left')\n",
    "    test = pd.merge(test, action_time_based_on_time, on='userid', how='left')\n",
    "    \n",
    "    util.log('act_time_window=' + str(act_time_window))\n",
    "    window = act_time_window\n",
    "    action_time_based_on_time_last_window = pd.read_csv('../data/output/feat/%s%d' % ('action_time_based_on_time_last_window', window))\n",
    "    train = pd.merge(train, action_time_based_on_time_last_window, on='userid', how='left')\n",
    "    test = pd.merge(test, action_time_based_on_time_last_window, on='userid', how='left')\n",
    "    \n",
    "#     util.log('act_time_row_stat_window=' + str(act_time_row_stat_window))\n",
    "#     window = act_time_row_stat_window\n",
    "#     action_time_row_stat_based_on_time_last_window = pd.read_csv('../data/output/feat/%s%d' % ('action_time_row_stat_based_on_time_last_window', window))\n",
    "#     train = pd.merge(train, action_time_row_stat_based_on_time_last_window, on='userid', how='left')\n",
    "#     test = pd.merge(test, action_time_row_stat_based_on_time_last_window, on='userid', how='left')\n",
    " \n",
    "#     util.log('action_time_diff2_window=' + str(action_time_diff2_window))\n",
    "#     window = action_time_diff2_window\n",
    "#     action_time_diff2_based_on_time_last_window = pd.read_csv('../data/output/feat/%s%d' % ('action_time_diff2_based_on_time_last_window', window))\n",
    "#     train = pd.merge(train, action_time_diff2_based_on_time_last_window, on='userid', how='left')\n",
    "#     test = pd.merge(test, action_time_diff2_based_on_time_last_window, on='userid', how='left')\n",
    "\n",
    "    util.log('act_time_1type_window=%d' % act_time_1type_window)\n",
    "    window = act_time_1type_window\n",
    "    for ttype in [1, 5, 6, 7, 8, 9]:\n",
    "        action_time_based_on_time_last_window_on_type = pd.read_csv('../data/output/feat/%s%d%s%d' % ('action_time_based_on_time_last_window', window, '_on_type', ttype))\n",
    "        train = pd.merge(train, action_time_based_on_time_last_window_on_type, on='userid', how='left')\n",
    "        test = pd.merge(test, action_time_based_on_time_last_window_on_type, on='userid', how='left')\n",
    "    \n",
    "#     util.log('action_time_2order_window=' + str(action_time_2order_window))\n",
    "#     window = action_time_2order_window\n",
    "#     action_time_2order_based_on_time_last_window = pd.read_csv('../data/output/feat/%s%d' % ('action_time_2order_based_on_time_last_window', window))\n",
    "#     train = pd.merge(train, action_time_2order_based_on_time_last_window, on='userid', how='left')\n",
    "#     test = pd.merge(test, action_time_2order_based_on_time_last_window, on='userid', how='left')\n",
    "\n",
    "#     util.log('act_real_time_1type_window=%d' % act_real_time_1type_window)\n",
    "#     window = act_real_time_1type_window\n",
    "#     for ttype in [1, 5, 6, 7, 8, 9]:\n",
    "#         action_real_time_based_on_time_last_window_on_type = pd.read_csv('../data/output/feat/%s%d%s%d' % ('action_real_time_based_on_time_last_window', window, '_on_type', ttype))\n",
    "#         train = pd.merge(train, action_real_time_based_on_time_last_window_on_type, on='userid', how='left')\n",
    "#         test = pd.merge(test, action_real_time_based_on_time_last_window_on_type, on='userid', how='left')\n",
    "\n",
    "#     action_order_time_diff = pd.read_csv('../data/output/feat/%s' % 'action_order_time_diff')\n",
    "#     train = pd.merge(train, action_order_time_diff, on='userid', how='left')\n",
    "#     test = pd.merge(test, action_order_time_diff, on='userid', how='left')\n",
    "\n",
    "#     order_last_order_ydm = pd.read_csv('../data/output/feat/%s' % 'order_last_order_ydm')\n",
    "#     train = pd.merge(train, order_last_order_ydm, on='userid', how='left')\n",
    "#     test = pd.merge(test, order_last_order_ydm, on='userid', how='left')\n",
    "\n",
    "    order_type1_ydm = pd.read_csv('../data/output/feat/%s' % 'order_type1_ydm')\n",
    "    train = pd.merge(train, order_type1_ydm, on='userid', how='left')\n",
    "    test = pd.merge(test, order_type1_ydm, on='userid', how='left')\n",
    "\n",
    "    util.log('act_ord_act_time_diff_window=' + str(act_ord_act_time_diff_window))\n",
    "    window = act_ord_act_time_diff_window\n",
    "    act_ord_act_time_diff_last_window = pd.read_csv('../data/output/feat/%s%d' % ('act_ord_act_time_diff_last_window', window))\n",
    "    train = pd.merge(train, act_ord_act_time_diff_last_window, on='userid', how='left')\n",
    "    test = pd.merge(test, act_ord_act_time_diff_last_window, on='userid', how='left')\n",
    "\n",
    "#     util.log('act_ord_type1_act_time_diff_window=' + str(act_ord_type1_act_time_diff_window))\n",
    "#     window = act_ord_type1_act_time_diff_window\n",
    "#     act_ord_type1_act_time_diff_last_window = pd.read_csv('../data/output/feat/%s%d' % ('act_ord_type1_act_time_diff_last_window', window))\n",
    "#     train = pd.merge(train, act_ord_type1_act_time_diff_last_window, on='userid', how='left')\n",
    "#     test = pd.merge(test, act_ord_type1_act_time_diff_last_window, on='userid', how='left')\n",
    "\n",
    "    util.log('action_sequence_time_diff_window=' + str(action_sequence_time_diff_window))\n",
    "    window = action_sequence_time_diff_window\n",
    "    action_sequence_time_diff_window = pd.read_csv('../data/output/feat/%s%d' % ('action_sequence_time_diff_window', window))\n",
    "    train = pd.merge(train, action_sequence_time_diff_window, on='userid', how='left')\n",
    "    test = pd.merge(test, action_sequence_time_diff_window, on='userid', how='left')\n",
    "\n",
    "#     action_sequence_time_stat_last123 = pd.read_csv('../data/output/feat/%s' % 'action_sequence_time_stat_last123')\n",
    "#     train = pd.merge(train, action_sequence_time_stat_last123, on='userid', how='left')\n",
    "#     test = pd.merge(test, action_sequence_time_stat_last123, on='userid', how='left')\n",
    "\n",
    "    util.log('action_time_diff_234_56789_window=' + str(action_time_diff_234_56789_window))\n",
    "    window = action_time_diff_234_56789_window\n",
    "    action_time_diff_234_56789_last_window = pd.read_csv('../data/output/feat/%s%d' % ('action_time_diff_234_56789_last_window', window))\n",
    "    train = pd.merge(train, action_time_diff_234_56789_last_window, on='userid', how='left')\n",
    "    test = pd.merge(test, action_time_diff_234_56789_last_window, on='userid', how='left')\n",
    "    \n",
    "#     action_stat_last_every_type = pd.read_csv('../data/output/feat/%s' % 'action_stat_last_every_type')\n",
    "#     train = pd.merge(train, action_stat_last_every_type, on='userid', how='left')\n",
    "#     test = pd.merge(test, action_stat_last_every_type, on='userid', how='left')\n",
    "\n",
    "#     act_ord_before_type1_stat = pd.read_csv('../data/output/feat/%s' % 'act_ord_before_type1_stat')\n",
    "#     train = pd.merge(train, act_ord_before_type1_stat, on='userid', how='left')\n",
    "#     test = pd.merge(test, act_ord_before_type1_stat, on='userid', how='left')\n",
    "\n",
    "    action_time_diff_stat = pd.read_csv('../data/output/feat/%s' % 'action_time_diff_stat')\n",
    "    train = pd.merge(train, action_time_diff_stat, on='userid', how='left')\n",
    "    test = pd.merge(test, action_time_diff_stat, on='userid', how='left')\n",
    "\n",
    "    util.log('action_time_diff_stat_window=' + str(action_time_diff_stat_window))\n",
    "    window = action_time_diff_stat_window\n",
    "    action_time_diff_stat_last_window = pd.read_csv('../data/output/feat/%s%d' % ('action_time_diff_stat_last_window', window))\n",
    "    train = pd.merge(train, action_time_diff_stat_last_window, on='userid', how='left')\n",
    "    test = pd.merge(test, action_time_diff_stat_last_window, on='userid', how='left')\n",
    "    \n",
    "#     action_time_last_on_every_type = pd.read_csv('../data/output/feat/%s' % 'action_time_last_on_every_type')\n",
    "#     train = pd.merge(train, action_time_last_on_every_type, on='userid', how='left')\n",
    "#     test = pd.merge(test, action_time_last_on_every_type, on='userid', how='left')\n",
    "\n",
    "    # bjw comment 中出现 order 中没有出现的为 1\n",
    "    bjw_train = pd.read_csv('../data/output/feat/bjw/train_fea.csv')\n",
    "    bjw_test = pd.read_csv('../data/output/feat/bjw/test_fea.csv')\n",
    "    train = pd.merge(train, bjw_train, on='userid', how='left')\n",
    "    test = pd.merge(test, bjw_test, on='userid', how='left')\n",
    "    \n",
    "    # 别人的开源特征，基于自己理解实现了一部分\n",
    "    tryy = pd.read_csv('../data/output/feat/%s' % 'try')\n",
    "    train = pd.merge(train, tryy, on='userid', how='left')\n",
    "    test = pd.merge(test, tryy, on='userid', how='left')\n",
    "    \n",
    "    # bjw 的特征\n",
    "    bjw_train = pd.read_csv('../data/output/feat/bjw/all_features_train.csv').drop(['Unnamed: 0', 'orderType'], axis=1)\n",
    "    bjw_train.columns = ['userid' if i == 0 else i for i in range(len(bjw_train.columns))]\n",
    "    bjw_test = pd.read_csv('../data/output/feat/bjw/all_features_test.csv').drop(['Unnamed: 0'], axis=1)\n",
    "    bjw_test.columns = ['userid' if i == 0 else i for i in range(len(bjw_test.columns))]\n",
    "    train = pd.merge(train, bjw_train, on='userid', how='left')\n",
    "    test = pd.merge(test, bjw_test, on='userid', how='left')\n",
    "    \n",
    "#################################################################################################################\n",
    "    \n",
    "    # 用于交叉特征，使用之后会移除\n",
    "    window = 1\n",
    "    for ttype in [1, 2, 3, 4, 5, 6, 7, 8, 9]:\n",
    "        action_real_time_based_on_time_last_window_on_type = pd.read_csv('../data/output/feat/%s%d%s%d' % ('action_real_time_based_on_time_last_window', window, '_on_type', ttype))\n",
    "        train = pd.merge(train, action_real_time_based_on_time_last_window_on_type, on='userid', how='left')\n",
    "        test = pd.merge(test, action_real_time_based_on_time_last_window_on_type, on='userid', how='left')\n",
    "\n",
    "    train, test = cross_feature(train, test)\n",
    "    \n",
    "    train, test = drop_duplicate_column(train, test)\n",
    "    \n",
    "    train_feature = train.drop(['orderType'], axis = 1)\n",
    "    train_label = train.orderType.values\n",
    "    test_feature = test\n",
    "    test_index = test.userid.values\n",
    "    \n",
    "    return train_feature, train_label, test_feature, test_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_feature(train, test):\n",
    "    util.log('Cross feature...')\n",
    "    \n",
    "    # 最近的 action 与最近的 order 的时间差\n",
    "    train['act_last_time-ord_last_time'] = train['act_last_time'] - train['ord_last_time']\n",
    "    train['act_last_time-ord_type0_time_max'] = train['act_last_time'] - train['ord_type0_time_max']\n",
    "    train['act_last_time-ord_type1_time_max'] = train['act_last_time'] - train['ord_type1_time_max']\n",
    "    test['act_last_time-ord_last_time'] = test['act_last_time'] - test['ord_last_time']\n",
    "    test['act_last_time-ord_type0_time_max'] = test['act_last_time'] - test['ord_type0_time_max']\n",
    "    test['act_last_time-ord_type1_time_max'] = test['act_last_time'] - test['ord_type1_time_max']\n",
    "    \n",
    "    # 最早的 action 与最早的 order 的时间差\n",
    "    train['act_first_time-ord_first_time'] = train['act_first_time'] - train['ord_first_time']\n",
    "    train['act_first_time-ord_type0_time_min'] = train['act_first_time'] - train['ord_type0_time_min']\n",
    "    train['act_first_time-ord_type1_time_min'] = train['act_first_time'] - train['ord_type1_time_min']\n",
    "    test['act_first_time-ord_first_time'] = test['act_first_time'] - test['ord_first_time']\n",
    "    test['act_first_time-ord_type0_time_min'] = test['act_first_time'] - test['ord_type0_time_min']\n",
    "    test['act_first_time-ord_type1_time_min'] = test['act_first_time'] - test['ord_type1_time_min']\n",
    "    \n",
    "    # 最近的 action 与最近的每一个 type 的 action 的时间差 + 最早的 action 与最早的每一个 type 的 action 的时间差\n",
    "    for ttype in [1, 2, 3, 4, 5, 6, 7, 8, 9]:\n",
    "        train['act_last_time-act_time(rank_1)(window_1)(type_%d)' % ttype] = train['act_last_time'] - train['act_time(rank_1)(window_1)(type_%d)' % ttype]\n",
    "        train['act_first_time-act_time(rank_1)(window_1)(type_%d)' % ttype] = train['act_first_time'] - train['act_time(rank_1)(window_1)(type_%d)' % ttype]\n",
    "        test['act_last_time-act_time(rank_1)(window_1)(type_%d)' % ttype] = test['act_last_time'] - test['act_time(rank_1)(window_1)(type_%d)' % ttype]\n",
    "        test['act_first_time-act_time(rank_1)(window_1)(type_%d)' % ttype] = test['act_first_time'] - test['act_time(rank_1)(window_1)(type_%d)' % ttype]\n",
    "        train = train.drop(['act_time(rank_1)(window_1)(type_%d)' % ttype], axis=1)\n",
    "        test = test.drop(['act_time(rank_1)(window_1)(type_%d)' % ttype], axis=1)\n",
    "\n",
    "    # 是否下过精品服务的单 * 最近的 action 的时间\n",
    "    tmp = train['ord_num(type_1)'].copy()\n",
    "    tmp[tmp > 1] = 1\n",
    "    tmp = pd.get_dummies(tmp.fillna(-1))\n",
    "    tmp.columns = ['has_ord_serv_nan', 'has_ord_serv_no', 'has_ord_serv_yes']\n",
    "    train = pd.concat([train, tmp.mul(train['act_last_time'], axis=0)], axis=1)\n",
    "    tmp = test['ord_num(type_1)'].copy()\n",
    "    tmp[tmp > 1] = 1\n",
    "    tmp = pd.get_dummies(tmp.fillna(-1))\n",
    "    tmp.columns = ['has_ord_serv_nan', 'has_ord_serv_no', 'has_ord_serv_yes']\n",
    "    test = pd.concat([test, tmp.mul(test['act_last_time'], axis=0)], axis=1)\n",
    "    \n",
    "    # 是否下过精品服务的单 * 每一个 type 的 action 的数量\n",
    "    tmp = train['ord_num(type_1)'].copy()\n",
    "    tmp[tmp > 1] = 1\n",
    "    tmp = pd.get_dummies(tmp.fillna(-1))\n",
    "    tmp.columns = ['has_ord_serv_nan', 'has_ord_serv_no', 'has_ord_serv_yes']\n",
    "    for ttype in [1, 2, 3, 4, 5, 6, 7, 8, 9]:\n",
    "        train = train.join(tmp.mul(train['act_num(type_%d)' % ttype], axis=0), rsuffix='*act_num(type_%d)' % ttype)\n",
    "    tmp = test['ord_num(type_1)'].copy()\n",
    "    tmp[tmp > 1] = 1\n",
    "    tmp = pd.get_dummies(tmp.fillna(-1))\n",
    "    tmp.columns = ['has_ord_serv_nan', 'has_ord_serv_no', 'has_ord_serv_yes']\n",
    "    for ttype in [1, 2, 3, 4, 5, 6, 7, 8, 9]:\n",
    "        test = test.join(tmp.mul(test['act_num(type_%d)' % ttype], axis=0), rsuffix='*act_num(type_%d)' % ttype)\n",
    "        \n",
    "#     # 最近的 order 与最近的每一个 type 的 action 的时间差 + 最早的 order 与最早的每一个 type 的 action 的时间差 （all/0/1）\n",
    "#     for ttype in [1, 2, 3, 4, 5, 6, 7, 8, 9]:\n",
    "#         train['ord_last_time-act_time(rank_1)(window_1)(type_%d)' % ttype] = train['ord_last_time'] -  train['act_time(rank_1)(window_1)(type_%d)' % ttype]\n",
    "#         train['ord_type0_time_max-act_time(rank_1)(window_1)(type_%d)' % ttype] = train['ord_type0_time_max'] - train['act_time(rank_1)(window_1)(type_%d)' % ttype]\n",
    "#         train['ord_type1_time_max-act_time(rank_1)(window_1)(type_%d)' % ttype] = train['ord_type1_time_max'] - train['act_time(rank_1)(window_1)(type_%d)' % ttype]\n",
    "#         train['ord_first_time-act_time(rank_1)(window_1)(type_%d)' % ttype] = train['ord_first_time'] - train['act_time(rank_1)(window_1)(type_%d)' % ttype]\n",
    "#         train['ord_type0_time_min-act_time(rank_1)(window_1)(type_%d)' % ttype] = train['ord_type0_time_min'] - train['act_time(rank_1)(window_1)(type_%d)' % ttype]\n",
    "#         train['ord_type1_time_min-act_time(rank_1)(window_1)(type_%d)' % ttype] = train['ord_type1_time_min'] - train['act_time(rank_1)(window_1)(type_%d)' % ttype]\n",
    "#         test['ord_last_time-act_time(rank_1)(window_1)(type_%d)' % ttype] = test['ord_last_time'] -  test['act_time(rank_1)(window_1)(type_%d)' % ttype]\n",
    "#         test['ord_type0_time_max-act_time(rank_1)(window_1)(type_%d)' % ttype] = test['ord_type0_time_max'] - test['act_time(rank_1)(window_1)(type_%d)' % ttype]\n",
    "#         test['ord_type1_time_max-act_time(rank_1)(window_1)(type_%d)' % ttype] = test['ord_type1_time_max'] - test['act_time(rank_1)(window_1)(type_%d)' % ttype]\n",
    "#         test['ord_first_time-act_time(rank_1)(window_1)(type_%d)' % ttype] = test['ord_first_time'] - test['act_time(rank_1)(window_1)(type_%d)' % ttype]\n",
    "#         test['ord_type0_time_min-act_time(rank_1)(window_1)(type_%d)' % ttype] = test['ord_type0_time_min'] - test['act_time(rank_1)(window_1)(type_%d)' % ttype]\n",
    "#         test['ord_type1_time_min-act_time(rank_1)(window_1)(type_%d)' % ttype] = test['ord_type1_time_min'] - test['act_time(rank_1)(window_1)(type_%d)' % ttype]\n",
    "        \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def drop_duplicate_column(train, test):\n",
    "    util.log('Drop duplicate column...')\n",
    "    \n",
    "    train = train.drop(['act_type(rank_1)(window6)'], axis=1)  # window9\n",
    "    test = test.drop(['act_type(rank_1)(window6)'], axis=1)\n",
    "    \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lgb_cv(train_feature, train_label, params, folds, rounds):\n",
    "    start = time.clock()\n",
    "    print train_feature.columns\n",
    "    dtrain = lgb.Dataset(train_feature, label=train_label)\n",
    "    num_round = rounds\n",
    "    print 'run cv: ' + 'round: ' + str(rounds)\n",
    "    res = lgb.cv(params, dtrain, num_round, nfold=folds, verbose_eval=20, early_stopping_rounds=100)\n",
    "    elapsed = (time.clock() - start)\n",
    "    print 'Time used:', elapsed, 's'\n",
    "    return len(res['auc-mean']), res['auc-mean'][len(res['auc-mean']) - 1]\n",
    "\n",
    "\n",
    "def lgb_predict(train_feature, train_label, test_feature, rounds, params):\n",
    "    dtrain = lgb.Dataset(train_feature, label=train_label)\n",
    "    valid_sets = [dtrain]\n",
    "    num_round = rounds\n",
    "    model = lgb.train(params, dtrain, num_round, valid_sets, verbose_eval=50)\n",
    "    predict = model.predict(test_feature)\n",
    "    return model, predict\n",
    "\n",
    "\n",
    "def store_result(test_index, pred, name):\n",
    "    result = pd.DataFrame({'userid': test_index, 'orderType': pred})\n",
    "    result.to_csv('../data/output/sub/' + name + '.csv', index=0, columns=['userid', 'orderType'])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feature, train_label, test_feature, test_index = merge_feature(6, 6, 3, 6, 6, 6, 6, 6, 6, 3)\n",
    "print train_feature.shape, train_label.shape, test_feature.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    'rounds': 10000,\n",
    "    'folds': 5\n",
    "}\n",
    "\n",
    "params_lgb = {\n",
    "    'task': 'train',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'binary',\n",
    "    'metric': 'auc',\n",
    "    'min_sum_hessian_in_leaf': 0.1,\n",
    "    'learning_rate': 0.01,\n",
    "    'verbosity': 2,\n",
    "    'tree_learner': 'feature',\n",
    "    'num_leaves': 128,\n",
    "    'feature_fraction': 0.75,\n",
    "    'bagging_fraction': 0.9,\n",
    "    'bagging_freq': 1,\n",
    "    'num_threads': 16,\n",
    "    'seed': 7\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "iterations, best_score = lgb_cv(train_feature, train_label, params_lgb, config['folds'], config['rounds'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preds = 0\n",
    "for s in range(7, 11):\n",
    "    params_lgb['seed'] = s\n",
    "    model, pred = lgb_predict(train_feature, train_label, test_feature, iterations, params_lgb)\n",
    "    preds += pred\n",
    "preds /= 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = store_result(test_index, preds, '20180207-lgb-%f(r%d)' % (best_score, iterations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\".join((\"%s: %.2f\" % x) for x in sorted(zip(train_feature.columns, model.feature_importance(\"gain\")), key=lambda x: x[1], reverse=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######################################### blending #########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test1 = pd.read_csv('../data/output/sub/bjw/result_addUserid_0125_1.csv')\n",
    "test2 = pd.read_csv('../data/output/sub/20180203-lgb-0.966497(r1843).csv')\n",
    "test3 = pd.read_csv('../data/output/sub/shawn_lgb_local9641_online9646.csv')\n",
    "test4 = pd.read_csv('../data/output/sub/ym/lz96490.csv')\n",
    "testa = pd.merge(test1, test2, on='userid', how='left')\n",
    "testb = pd.merge(test3, test4, on='userid', how='left')\n",
    "test = pd.merge(testa, testb, on='userid', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test['orderType'] = 0.5 * test['orderType_x_x'] + 0.3 * test['orderType_y_x'] + 0.1 * test['orderType_x_y'] + 0.1 * test['orderType_y_y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test[['userid','orderType']].to_csv('../data/output/sub/blend/20180203-0.5bjw+0.3+0.1+0.1ym.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
