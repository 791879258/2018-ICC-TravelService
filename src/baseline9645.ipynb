{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import time\n",
    "import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_tr = pd.read_csv('../data/input/train/action_train.csv')  # 用户行为数据\n",
    "order_future_tr = pd.read_csv('../data/input/train/orderFuture_train.csv')  # 待预测数据\n",
    "order_history_tr = pd.read_csv('../data/input/train/orderHistory_train.csv')  # 用户历史订单数据\n",
    "user_comment_tr = pd.read_csv('../data/input/train/userComment_train.csv')  # 用户评论数据\n",
    "user_profile_tr = pd.read_csv('../data/input/train/userProfile_train.csv')  # 用户个人信息\n",
    "\n",
    "action_te = pd.read_csv('../data/input/test/action_test.csv')\n",
    "order_future_te = pd.read_csv('../data/input/test/orderFuture_test.csv')\n",
    "order_history_te = pd.read_csv('../data/input/test/orderHistory_test.csv')\n",
    "user_comment_te = pd.read_csv('../data/input/test/userComment_test.csv')\n",
    "user_profile_te = pd.read_csv('../data/input/test/userProfile_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "action = pd.concat([action_tr, action_te], axis=0).reset_index(drop=True)\n",
    "order_history = pd.concat([order_history_tr, order_history_te], axis=0).reset_index(drop=True)\n",
    "user_comment = pd.concat([user_comment_tr, user_comment_te], axis=0).reset_index(drop=True)\n",
    "user_profile = pd.concat([user_profile_tr, user_profile_te], axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_user_profile_feature(df):\n",
    "    le = preprocessing.LabelBinarizer()\n",
    "    encoder = le.fit_transform(df.gender.fillna('_NA_').values)\n",
    "    encoder_col = ['gender_%d'%i for i in range(encoder.shape[1])]\n",
    "    df1 = pd.DataFrame(encoder, columns = encoder_col)\n",
    "    df1['userid'] = df['userid'].values\n",
    "    f = df1\n",
    "    \n",
    "    le = preprocessing.LabelBinarizer()\n",
    "    encoder = le.fit_transform(df.province.fillna('_NA_').values)\n",
    "    encoder_col = ['province_%d'%i for i in range(encoder.shape[1])]\n",
    "    df1 = pd.DataFrame(encoder, columns = encoder_col)\n",
    "    df1['userid'] = df['userid'].values\n",
    "    f = pd.merge(f,df1, on = 'userid', how = 'left')\n",
    "    \n",
    "    le = preprocessing.LabelBinarizer()\n",
    "    encoder = le.fit_transform(df.age.fillna('_NA_').values)\n",
    "    encoder_col = ['age_%d'%i for i in range(encoder.shape[1])]\n",
    "    df1 = pd.DataFrame(encoder, columns = encoder_col)\n",
    "    df1['userid'] = df['userid'].values\n",
    "    f = pd.merge(f,df1, on = 'userid', how = 'left')\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_action_feature(data):\n",
    "    df = copy.deepcopy(data)\n",
    "    #---all action feature\n",
    "    result = df.groupby(['userid','actionType'])['actionTime'].count().reset_index().rename(\n",
    "        columns = {'actionTime':'actionNum'}).pivot('userid','actionType','actionNum').apply(lambda x: x/np.sum(x))\n",
    "    result.columns = ['action_type_num%d'%i for i in range(result.shape[1])]\n",
    "    result = result.reset_index()\n",
    "    \n",
    "    #---get feature by unix time\n",
    "    for window in [6]:\n",
    "        print window\n",
    "        print 'actiontype'\n",
    "        f = df.groupby('userid').apply(lambda x: x.sort_values('actionTime', ascending = False).head(window)).reset_index(drop = True).rename(\n",
    "            columns={'actionType': 'actionType_last', 'actionTime': 'actionTime_last'})\n",
    "        f2 = pd.concat([f,f.groupby('userid').rank(method = 'first').astype('int').reset_index().rename(\n",
    "                columns={'actionTime_last': 'actionTime_last_rank'})['actionTime_last_rank']],axis = 1)\n",
    "        \n",
    "        ff1 = f2.pivot('userid','actionTime_last_rank','actionType_last')\n",
    "        ff1_diff = ff1.diff(1, axis = 1)\n",
    "        ff1_diff.columns = ['last%d_type_1diff%d'%(window,i) for i in range(ff1_diff.shape[1])]\n",
    "        ff1_diff = ff1_diff.iloc[:,1:].reset_index()\n",
    "        result = pd.merge(result, ff1_diff, on = 'userid', how = 'left')\n",
    "        \n",
    "        ff1['last%d_type_max'%window] = ff1.max(axis = 0)\n",
    "        ff1['last%d_type_min'%window] = ff1.min(axis = 0)\n",
    "        ff1['last%d_type_meam'%window] = ff1.mean(axis = 0)\n",
    "        ff1['last%d_type_median'%window] = ff1.median(axis = 0)\n",
    "        ff1['last%d_type_std'%window] = ff1.std(axis = 0)\n",
    "        ff1['last%d_type_sum'%window] = ff1.sum(axis = 0)\n",
    "        \n",
    "        ff1.columns = ['last%d_type%d'%(window,i) for i in range(ff1.shape[1])]\n",
    "        ff1 = ff1.reset_index()\n",
    "        \n",
    "        result = pd.merge(result, ff1, on = 'userid', how = 'left')\n",
    "        \n",
    "\n",
    "        print 'actiontime'\n",
    "        ff2 = f2.pivot('userid','actionTime_last_rank','actionTime_last')\n",
    "        ff2_diff = ff2.diff(1, axis = 1)\n",
    "        ff2_diff.columns = ['last%d_time_1diff%d'%(window,i) for i in range(ff2_diff.shape[1])]\n",
    "        ff2_diff = ff2_diff.iloc[:,1:].reset_index()\n",
    "        result = pd.merge(result, ff2_diff, on = 'userid', how = 'left')\n",
    "        \n",
    "        \n",
    "\n",
    "        ff2['last%d_time_max'%window] = ff2.max(axis = 0)\n",
    "        ff2['last%d_time_min'%window] = ff2.min(axis = 0)\n",
    "        ff2['last%d_time_meam'%window] = ff2.mean(axis = 0)\n",
    "        ff2['last%d_time_median'%window] = ff2.median(axis = 0)\n",
    "        ff2['last%d_time_std'%window] = ff2.std(axis = 0)\n",
    "        ff2['last%d_time_sum'%window] = ff2.sum(axis = 0)\n",
    "        \n",
    "        ff2.columns = ['last%d_time%d'%(window,i) for i in range(ff2.shape[1])]\n",
    "        ff2 = ff2.reset_index()\n",
    "        result = pd.merge(result, ff2, on = 'userid', how = 'left')\n",
    "       \n",
    "        \n",
    "        ff = f.groupby(['userid','actionType_last'])['actionTime_last'].count().reset_index().rename(\n",
    "            columns = {'actionTime_last':'actionNum_last'}).pivot('userid','actionType_last','actionNum_last').apply(lambda x: x/np.sum(x))\n",
    "        ff.columns = ['last%d_action_type_num%d'%(window,i) for i in range(ff.shape[1])]\n",
    "        ff = ff.reset_index()\n",
    "        result = pd.merge(result, ff, on = 'userid', how = 'left')\n",
    "        \n",
    "    #---sort every type last 1 action feature\n",
    "    f = df.groupby(['userid','actionType']).apply(lambda x: x.sort_values('actionTime', ascending = False).head(1)).reset_index(drop = True).rename(\n",
    "        columns={'actionTime': 'type_actionTime_last'})\n",
    "    ff3 = f.pivot('userid','actionType','type_actionTime_last')\n",
    "    \n",
    "    ff3_diff = ff3.diff(1, axis = 1)\n",
    "    ff3_diff.columns = ['type_%d_lsttime_diff'%i for i in range(ff3_diff.shape[1])]\n",
    "    ff3_diff = ff3_diff.iloc[:,1:].reset_index()\n",
    "    ff3.columns = ['type_%d_lasttime'%i for i in range(ff3.shape[1])]\n",
    "    ff3 = ff3.reset_index()\n",
    "    result = pd.merge(result, ff3, on = 'userid', how = 'left')\n",
    "    \n",
    "    for t in [1,2,3,4,5,6,7,8,9]:\n",
    "        print t\n",
    "        window = 5\n",
    "        df_type = df[df.actionType == t]\n",
    "        f = df_type.groupby('userid').apply(lambda x: x.sort_values('actionTime', ascending = False).head(window)).reset_index(drop = True).rename(\n",
    "            columns={'actionTime': 'actionTime_last'})\n",
    "        f2 = pd.concat([f,f.groupby('userid').rank(method = 'first').astype('int').reset_index().rename(\n",
    "                columns={'actionTime_last': 'actionTime_last_rank'})['actionTime_last_rank']],axis = 1)\n",
    "        \n",
    "        ff1 = f2.pivot('userid','actionTime_last_rank','actionTime_last')\n",
    "        ff1_diff = ff1.diff(1, axis = 1)\n",
    "        ff1_diff.columns = ['last%d_type%d_time_diff%d'%(window,t,i) for i in range(ff1_diff.shape[1])]\n",
    "        ff1_diff = ff1_diff.iloc[:,1:].reset_index()\n",
    "        \n",
    "        ff1['last%d_type%d_max'%(window,t)] = ff1.max(axis = 0)\n",
    "        ff1['last%d_type%d_min'%(window,t)] = ff1.min(axis = 0)\n",
    "        ff1['last%d_type%d_meam'%(window,t)] = ff1.mean(axis = 0)\n",
    "        ff1['last%d_type%d_median'%(window,t)] = ff1.median(axis = 0)\n",
    "        ff1['last%d_type%d_std'%(window,t)] = ff1.std(axis = 0)\n",
    "        ff1['last%d_type%d_sum'%(window,t)] = ff1.sum(axis = 0)\n",
    "        \n",
    "        ff1.columns = ['last%d_type%d_time%d'%(window,t,i) for i in range(ff1.shape[1])]\n",
    "        ff1 = ff1.reset_index()\n",
    "        \n",
    "        result = pd.merge(result, ff1, on = 'userid', how = 'left')\n",
    "        result = pd.merge(result, ff1_diff, on = 'userid', how = 'left')\n",
    "    #--get feature by date\n",
    "#     print 'date feature'\n",
    "#     df['date'] = pd.to_datetime(df['actionTime'],unit='s').dt.date\n",
    "#     for window in [40]:\n",
    "#         print window\n",
    "#         df_select = df[df.date >= pd.bdate_range(end=df.date.max(), periods=window).date[0]]\n",
    "#         print df_select.shape\n",
    "#         f = df_select.groupby('userid').apply(lambda x: x.sort_values('actionTime', ascending = False).head(6)).reset_index(drop = True).rename(\n",
    "#                 columns={'actionType': 'actionType_last', 'actionTime': 'actionTime_last'})\n",
    "#         f2 = pd.concat([f,f.groupby('userid').rank(method = 'first').astype('int').reset_index().rename(\n",
    "#                 columns={'actionTime_last': 'actionTime_last_rank'})['actionTime_last_rank']],axis = 1)\n",
    "        \n",
    "#         ff1 = f2.pivot('userid','actionTime_last_rank','actionType_last')\n",
    "#         ff1_diff = ff1.diff(1, axis = 1)\n",
    "#         ff1_diff.columns = ['lastdate%d_type_diff%d'%(window,i) for i in range(ff1_diff.shape[1])]\n",
    "#         ff1_diff = ff1_diff.iloc[:,1:].reset_index()\n",
    "        \n",
    "#         ff1['lastdate%d_type_max'%window] = ff1.max(axis = 0)\n",
    "#         ff1['lastdate%d_type_min'%window] = ff1.min(axis = 0)\n",
    "#         ff1['lastdate%d_type_meam'%window] = ff1.mean(axis = 0)\n",
    "#         ff1.columns = ['lastdate%d_type%d'%(window,i) for i in range(ff1.shape[1])]\n",
    "#         ff1 = ff1.reset_index()\n",
    "        \n",
    "#         result = pd.merge(result, ff1, on = 'userid', how = 'left')\n",
    "#         result = pd.merge(result, ff1_diff, on = 'userid', how = 'left')\n",
    "        \n",
    "#         ff2 = f2.pivot('userid','actionTime_last_rank','actionTime_last')\n",
    "#         ff2_diff = ff2.diff(1, axis = 1)\n",
    "#         ff2_diff.columns = ['last%d_time_diff%d'%(window,i) for i in range(ff2_diff.shape[1])]\n",
    "#         ff2_diff = ff2_diff.iloc[:,1:].reset_index()\n",
    "\n",
    "#         ff2['last%d_time_max'%window] = ff2.max(axis = 0)\n",
    "#         ff2['last%d_time_min'%window] = ff2.min(axis = 0)\n",
    "#         ff2['last%d_time_meam'%window] = ff2.mean(axis = 0)\n",
    "#         ff2.columns = ['last%d_time%d'%(window,i) for i in range(ff2.shape[1])]\n",
    "#         ff2 = ff2.reset_index()\n",
    "#         result = pd.merge(result, ff2, on = 'userid', how = 'left')\n",
    "#         result = pd.merge(result, ff2_diff, on = 'userid', how = 'left')\n",
    "        \n",
    "        \n",
    "#         ff = f.groupby(['userid','actionType_last'])['actionTime_last'].count().reset_index().rename(\n",
    "#             columns = {'actionTime_last':'actionNum_last'}).pivot('userid','actionType_last','actionNum_last').apply(lambda x: x/np.sum(x))\n",
    "#         ff.columns = ['last%d_action_type_num%d'%(window,i) for i in range(ff.shape[1])]\n",
    "#         ff = ff.reset_index()\n",
    "#         result = pd.merge(result, ff, on = 'userid', how = 'left')\n",
    "\n",
    "    \n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_order_history_feature(data):\n",
    "    le = preprocessing.LabelBinarizer()\n",
    "    encoder1 = le.fit(pd.concat([orderHistory_train,orderHistory_test]).country.values)\n",
    "\n",
    "    le = preprocessing.LabelBinarizer()\n",
    "    encoder2 = le.fit(pd.concat([orderHistory_train,orderHistory_test]).continent.values)\n",
    "    \n",
    "    df = copy.deepcopy(data)\n",
    "    #--get order feature\n",
    "    feature = df.groupby('userid')['orderType'].agg(['sum','count']).reset_index().rename(columns={'sum': 'order_num_1', 'count': 'order_num'})\n",
    "    feature['order_num_0'] = feature['order_num'] - feature['order_num_1']\n",
    "    feature['order_ratio_0'] = feature['order_num_0'].astype('float')/feature['order_num']\n",
    "    feature['order_ratio_1'] = feature['order_num_1'].astype('float')/feature['order_num']\n",
    "    f = copy.deepcopy(feature)\n",
    "    \n",
    "    #--get total feature\n",
    "    feature = df.groupby('userid')['city','country','continent'].count().reset_index().rename(\n",
    "        columns={'city': 'city_num', 'country': 'country_num','continent':'continent_num'})\n",
    "    f = pd.merge(f,feature, on = 'userid', how = 'left')\n",
    "    feature = df[df.orderType == 1].groupby('userid')['city','country','continent'].count().reset_index().rename(\n",
    "        columns={'city': 'city_num_1', 'country': 'country_num_1','continent':'continent_num_1'})\n",
    "    f = pd.merge(f,feature, on = 'userid', how = 'left').fillna(0)\n",
    "    for val in ['city_num', 'country_num', 'continent_num']:\n",
    "        f[val.split('_')[0]+'_ratio_1'] = f[val+'_1'].astype('float')/f[val]\n",
    "    #--get country feature\n",
    "#     le = preprocessing.LabelBinarizer()\n",
    "    country_encoder = encoder1.transform(df.country.values)\n",
    "    country_encoder_col = ['country_%d'%i for i in range(country_encoder.shape[1])]\n",
    "    df1 = pd.DataFrame(country_encoder, columns = country_encoder_col)\n",
    "    df1['userid'] = df['userid'].values\n",
    "    feature = df1.groupby('userid')[country_encoder_col].agg(['sum','count']).reset_index()\n",
    "    f = pd.merge(f,feature, on = 'userid', how = 'left')\n",
    "    \n",
    "    #--get continent feature\n",
    "#     le = preprocessing.LabelBinarizer()\n",
    "    continent_encoder = encoder2.transform(df.continent.values)\n",
    "    continent_encoder_col = ['continent_%d'%i for i in range(continent_encoder.shape[1])]\n",
    "    df1 = pd.DataFrame(continent_encoder, columns = continent_encoder_col)\n",
    "    df1['userid'] = df['userid'].values\n",
    "    feature = df1.groupby('userid')[continent_encoder_col].agg(['sum','count']).reset_index()\n",
    "    f = pd.merge(f,feature, on = 'userid', how = 'left')\n",
    "    \n",
    "    \n",
    "    #--get orderTime last feature\n",
    "#     df1 = df.groupby('userid').apply(lambda x: x.sort_values('orderTime', ascending = False).head(1)).reset_index(drop = True)[['userid','orderid','orderTime','orderType']]\n",
    "#     df1.columns = [['userid','last_orderid','last_orderTime','last_orderType']]\n",
    "#     f = pd.merge(f, df1, on = 'userid',how = 'left')\n",
    "    \n",
    "    #--get orderTime last 5 feature\n",
    "#     df1 = df.groupby('userid').apply(lambda x: x.sort_values('orderTime', ascending = False).head(5)).reset_index(drop = True)[['userid','orderid','orderTime','orderType']]\n",
    "#     df1.columns = [['userid','last_orderid','last_orderTime','last_orderType']]\n",
    "#     temp = pd.concat([df1,df1.groupby('userid').rank(method = 'first').astype('int').reset_index().rename(\n",
    "#             columns={'last_orderTime': 'last_orderTime_rank'})['last_orderTime_rank']],axis = 1)\n",
    "    \n",
    "#     ff1 = temp.pivot('userid','last_orderTime_rank','last_orderType')\n",
    "#     ff1.columns = ['his_type%d'%i for i in range(ff1.shape[1])]\n",
    "#     ff1 = ff1.reset_index()\n",
    "#     f = pd.merge(f, ff1, on = 'userid',how = 'left')\n",
    "    \n",
    "#     ff2 = temp.pivot('userid','last_orderTime_rank','last_orderTime')\n",
    "#     ff2.columns = ['his_time%d'%i for i in range(ff2.shape[1])]\n",
    "#     ff2 = ff2.reset_index()\n",
    "#     f = pd.merge(f, ff2, on = 'userid',how = 'left')\n",
    "    \n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_user_comment_feature(data):\n",
    "    df = copy.deepcopy(data)\n",
    "    feature = df.groupby('userid')['rating'].agg(['max','min','mean','sum','count','median','std']).reset_index().rename(\n",
    "        columns={'max': 'rate_max', 'min': 'rate_min','mean':'rate_mean','sum':'rate_sum','count':'rate_count','median':'rate_median','std':'rate_std'})\n",
    "\n",
    "#     mlb = MultiLabelBinarizer()\n",
    "#     tagsV = []\n",
    "#     for line in df.tags.values:\n",
    "#         if line == line:\n",
    "# #             print line\n",
    "#             tagsV.append(set(line.split('|')))\n",
    "#         else:\n",
    "#             tagsV.append(set(''))\n",
    "            \n",
    "    \n",
    "#     tagsF = mlb.fit_transform(tagsV)\n",
    "#     name = []\n",
    "#     for i in range(tagsF.shape[1]):\n",
    "#         df['tag_%d'%i] = tagsF[:,i]\n",
    "#         name.append('tag_%d'%i)\n",
    "    \n",
    "#     f = df.groupby('userid')[name].agg(['mean','sum','count','std']).reset_index()\n",
    "    \n",
    "#     feature = pd.merge(feature, f, on= 'userid', how = 'left') \n",
    "    \n",
    "    \n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_profile_feat = get_user_profile_feature(user_profile)\n",
    "action_feat = get_action_feature(action)\n",
    "order_history_feat = get_order_history_feature(order_history)\n",
    "user_comment_feat = get_user_comment_feature(user_comment)\n",
    "\n",
    "dataset = pd.merge(user_profile_feat, action_feat, on='userid', how='left')\n",
    "dataset = pd.merge(dataset, order_history_feat, on='userid', how='left')\n",
    "dataset = pd.merge(dataset, user_comment_feat, on='userid', how='left')\n",
    "\n",
    "trainset = pd.merge(order_future_tr, dataset, on='userid', how='left')\n",
    "testset = pd.merge(order_future_te, dataset, on='userid', how='left')\n",
    "\n",
    "train_feature = trainset.drop(['orderType'], axis=1)\n",
    "train_label = trainset.orderType.values\n",
    "test_feature = testset\n",
    "test_index = testset.userid.values\n",
    "\n",
    "print train_feature.shape, train_label.shape\n",
    "print test_feature.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    'rounds': 10000,\n",
    "    'folds': 5\n",
    "}\n",
    "\n",
    "params = {\n",
    "    'task': 'train',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'binary',\n",
    "    'metric': 'auc',\n",
    "    'min_sum_hessian_in_leaf': 0.1,\n",
    "    'learning_rate': 0.01,\n",
    "    'verbosity': 2,\n",
    "    'tree_learner': 'feature',\n",
    "    'num_leaves': 128,\n",
    "    'feature_fraction': 0.7,\n",
    "    'bagging_fraction': 0.7,\n",
    "    'bagging_freq': 1,\n",
    "    'num_threads': 16,\n",
    "    'seed': 20\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lgb_cv(train_feature, train_label, params, folds, rounds):\n",
    "    start = time.clock()\n",
    "    print train_feature.columns\n",
    "    dtrain = lgb.Dataset(train_feature, label=train_label)\n",
    "    num_round = rounds\n",
    "    print 'run cv: ' + 'round: ' + str(rounds)\n",
    "    res = lgb.cv(params, dtrain, num_round, nfold=folds, verbose_eval=20, early_stopping_rounds=100)\n",
    "    elapsed = (time.clock() - start)\n",
    "    print 'Time used:', elapsed, 's'\n",
    "    return len(res['auc-mean']), res['auc-mean'][len(res['auc-mean']) - 1]\n",
    "\n",
    "def lgb_predict(train_feature, train_label, test_feature, rounds, params):\n",
    "    dtrain = lgb.Dataset(train_feature, label=train_label)\n",
    "    valid_sets = [dtrain]\n",
    "    num_round = rounds\n",
    "    model = lgb.train(params, dtrain, num_round, valid_sets, verbose_eval=50)\n",
    "    predict = model.predict(test_feature)\n",
    "    return model, predict\n",
    "\n",
    "def store_result(test_index, pred, name):\n",
    "    result = pd.DataFrame({'userid': test_index, 'orderType': pred})\n",
    "    result.to_csv('../data/output/sub/' + name + '.csv', index=0, columns=['userid', 'orderType'])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([                 u'userid',                u'gender_0',\n",
      "                      u'gender_1',                u'gender_2',\n",
      "                    u'province_0',              u'province_1',\n",
      "                    u'province_2',              u'province_3',\n",
      "                    u'province_4',              u'province_5',\n",
      "       ...\n",
      "       (u'continent_4', u'count'),   (u'continent_5', u'sum'),\n",
      "       (u'continent_5', u'count'),                u'rate_max',\n",
      "                      u'rate_min',               u'rate_mean',\n",
      "                      u'rate_sum',              u'rate_count',\n",
      "                   u'rate_median',                u'rate_std'],\n",
      "      dtype='object', length=373)\n",
      "run cv: round: 10000\n",
      "[20]\tcv_agg's auc: 0.935916 + 0.00347213\n",
      "[40]\tcv_agg's auc: 0.939652 + 0.00393814\n",
      "[60]\tcv_agg's auc: 0.940786 + 0.00389518\n",
      "[80]\tcv_agg's auc: 0.942099 + 0.00350259\n",
      "[100]\tcv_agg's auc: 0.943578 + 0.00378149\n",
      "[120]\tcv_agg's auc: 0.945306 + 0.00356631\n",
      "[140]\tcv_agg's auc: 0.946317 + 0.0035726\n",
      "[160]\tcv_agg's auc: 0.947406 + 0.00346354\n",
      "[180]\tcv_agg's auc: 0.948506 + 0.00345494\n",
      "[200]\tcv_agg's auc: 0.949393 + 0.00344421\n",
      "[220]\tcv_agg's auc: 0.950714 + 0.00336369\n",
      "[240]\tcv_agg's auc: 0.95157 + 0.00341899\n",
      "[260]\tcv_agg's auc: 0.952276 + 0.00342924\n",
      "[280]\tcv_agg's auc: 0.953074 + 0.00333096\n",
      "[300]\tcv_agg's auc: 0.953735 + 0.00325356\n",
      "[320]\tcv_agg's auc: 0.954515 + 0.00320666\n",
      "[340]\tcv_agg's auc: 0.955291 + 0.00323589\n",
      "[360]\tcv_agg's auc: 0.955872 + 0.00319307\n",
      "[380]\tcv_agg's auc: 0.956316 + 0.00312056\n",
      "[400]\tcv_agg's auc: 0.956731 + 0.00316808\n",
      "[420]\tcv_agg's auc: 0.957151 + 0.00321426\n",
      "[440]\tcv_agg's auc: 0.957563 + 0.00317496\n",
      "[460]\tcv_agg's auc: 0.957876 + 0.0031493\n",
      "[480]\tcv_agg's auc: 0.958305 + 0.003101\n",
      "[500]\tcv_agg's auc: 0.958495 + 0.00308035\n",
      "[520]\tcv_agg's auc: 0.958757 + 0.00314344\n",
      "[540]\tcv_agg's auc: 0.958931 + 0.0032034\n",
      "[560]\tcv_agg's auc: 0.959191 + 0.00323489\n",
      "[580]\tcv_agg's auc: 0.959436 + 0.00322565\n",
      "[600]\tcv_agg's auc: 0.959618 + 0.00321968\n",
      "[620]\tcv_agg's auc: 0.959742 + 0.00321613\n",
      "[640]\tcv_agg's auc: 0.959883 + 0.00322608\n",
      "[660]\tcv_agg's auc: 0.959947 + 0.00325637\n",
      "[680]\tcv_agg's auc: 0.960088 + 0.00330481\n",
      "[700]\tcv_agg's auc: 0.960248 + 0.00335284\n",
      "[720]\tcv_agg's auc: 0.960367 + 0.00336506\n",
      "[740]\tcv_agg's auc: 0.960385 + 0.00339797\n",
      "[760]\tcv_agg's auc: 0.960449 + 0.00341312\n",
      "[780]\tcv_agg's auc: 0.960503 + 0.00343277\n",
      "[800]\tcv_agg's auc: 0.960543 + 0.00343752\n",
      "[820]\tcv_agg's auc: 0.960617 + 0.00343721\n",
      "[840]\tcv_agg's auc: 0.960638 + 0.00346683\n",
      "[860]\tcv_agg's auc: 0.960671 + 0.00345798\n",
      "[880]\tcv_agg's auc: 0.960697 + 0.00348162\n",
      "[900]\tcv_agg's auc: 0.960775 + 0.00350938\n",
      "[920]\tcv_agg's auc: 0.960785 + 0.00349945\n",
      "[940]\tcv_agg's auc: 0.960828 + 0.00350349\n",
      "[960]\tcv_agg's auc: 0.960855 + 0.0035045\n",
      "[980]\tcv_agg's auc: 0.960887 + 0.00349326\n",
      "[1000]\tcv_agg's auc: 0.960919 + 0.00351582\n",
      "[1020]\tcv_agg's auc: 0.960934 + 0.00352545\n",
      "[1040]\tcv_agg's auc: 0.96096 + 0.00353281\n",
      "[1060]\tcv_agg's auc: 0.960973 + 0.00354651\n",
      "[1080]\tcv_agg's auc: 0.960976 + 0.00354739\n",
      "[1100]\tcv_agg's auc: 0.960987 + 0.00352691\n",
      "[1120]\tcv_agg's auc: 0.960992 + 0.00354663\n",
      "[1140]\tcv_agg's auc: 0.961018 + 0.0035131\n",
      "[1160]\tcv_agg's auc: 0.961035 + 0.00351288\n",
      "[1180]\tcv_agg's auc: 0.961021 + 0.00351059\n",
      "[1200]\tcv_agg's auc: 0.961059 + 0.00350991\n",
      "[1220]\tcv_agg's auc: 0.961065 + 0.00350877\n",
      "[1240]\tcv_agg's auc: 0.96108 + 0.00352035\n",
      "[1260]\tcv_agg's auc: 0.961084 + 0.0035488\n",
      "[1280]\tcv_agg's auc: 0.961095 + 0.0035581\n",
      "[1300]\tcv_agg's auc: 0.961116 + 0.00353599\n",
      "[1320]\tcv_agg's auc: 0.961132 + 0.00355026\n",
      "[1340]\tcv_agg's auc: 0.961117 + 0.00354946\n",
      "[1360]\tcv_agg's auc: 0.961125 + 0.00353056\n",
      "[1380]\tcv_agg's auc: 0.961146 + 0.00351517\n",
      "[1400]\tcv_agg's auc: 0.961145 + 0.00352665\n",
      "[1420]\tcv_agg's auc: 0.96113 + 0.00354127\n",
      "[1440]\tcv_agg's auc: 0.96114 + 0.00355234\n",
      "[1460]\tcv_agg's auc: 0.961109 + 0.00355362\n",
      "[1480]\tcv_agg's auc: 0.961141 + 0.00355759\n",
      "Time used: 1015.93639555 s\n"
     ]
    }
   ],
   "source": [
    "iterations, best_score = lgb_cv(train_feature, train_label, params, config['folds'], config['rounds'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\ttraining's auc: 0.961372\n",
      "[100]\ttraining's auc: 0.966117\n",
      "[150]\ttraining's auc: 0.970849\n",
      "[200]\ttraining's auc: 0.974998\n",
      "[250]\ttraining's auc: 0.979141\n",
      "[300]\ttraining's auc: 0.982696\n",
      "[350]\ttraining's auc: 0.985932\n",
      "[400]\ttraining's auc: 0.988872\n",
      "[450]\ttraining's auc: 0.991118\n",
      "[500]\ttraining's auc: 0.993063\n",
      "[550]\ttraining's auc: 0.994594\n",
      "[600]\ttraining's auc: 0.995803\n",
      "[650]\ttraining's auc: 0.996739\n",
      "[700]\ttraining's auc: 0.997466\n",
      "[750]\ttraining's auc: 0.998034\n",
      "[800]\ttraining's auc: 0.998452\n",
      "[850]\ttraining's auc: 0.998802\n",
      "[900]\ttraining's auc: 0.999038\n",
      "[950]\ttraining's auc: 0.999239\n",
      "[1000]\ttraining's auc: 0.999403\n",
      "[1050]\ttraining's auc: 0.999536\n",
      "[1100]\ttraining's auc: 0.999624\n",
      "[1150]\ttraining's auc: 0.999705\n",
      "[1200]\ttraining's auc: 0.999772\n",
      "[1250]\ttraining's auc: 0.999825\n",
      "[1300]\ttraining's auc: 0.999866\n",
      "[1350]\ttraining's auc: 0.999895\n"
     ]
    }
   ],
   "source": [
    "model, pred = lgb_predict(train_feature, train_label, test_feature, iterations, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = store_result(test_index, pred, '20180121-lgb-%f(r%d)' % (best_score, iterations))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
